import os  # ìš´ì˜ì²´ì œ ê²½ë¡œ ë° íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ë¥¼ ìœ„í•œ pandas
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°ìš© numpy
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ Pillow
import torch  # PyTorch ë©”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.nn as nn  # ì‹ ê²½ë§ ë ˆì´ì–´ì™€ ì†ì‹¤í•¨ìˆ˜ ëª¨ë“ˆ
from torch.utils.data import Dataset, DataLoader, random_split  # ë°ì´í„°ì…‹ ë° ë¶„í• /ë¡œë” ìœ í‹¸ë¦¬í‹°
import torchvision.transforms as transforms  # ë°ì´í„° ì „ì²˜ë¦¬ ë³€í™˜ ëª¨ë“ˆ
import matplotlib.pyplot as plt  # ì‹œê°í™”ë¥¼ ìœ„í•œ matplotlib
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ ë°”
from model import SteeringModel, get_unique_train_folder  # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ê³¼ í•™ìŠµ í´ë” ìƒì„±ê¸°
from utils import SteeringDataset  # ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
from config import *  # ì„¤ì • ìƒìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

# í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train(model, loader, optimizer, criterion, device, clip_grad=None):
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    running_loss = 0.0  # ì—í­ ë™ì•ˆ ì†ì‹¤ ëˆ„ì  ë³€ìˆ˜
    for imgs, angles in tqdm(loader, desc="Training", leave=False):  # ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ ë£¨í”„
        imgs, angles = imgs.to(device), angles.to(device)  # ë°ì´í„°ë¥¼ GPU/CPUë¡œ ì´ë™
        optimizer.zero_grad()  # ì´ì „ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        outputs = model(imgs).squeeze(1) 
        loss = criterion(outputs, angles)
        #loss = criterion(model(imgs).squeeze(), angles)  # ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°ë„ì˜ ì†ì‹¤ ê³„ì‚°
        loss.backward()  # ì†ì‹¤ ì—­ì „íŒŒ
        # gradient clipping to avoid exploding gradients
        if clip_grad is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
        optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        running_loss += loss.item() * imgs.size(0)  # ë°°ì¹˜ ì†ì‹¤ ëˆ„ì 
    return running_loss / len(loader.dataset)  # í‰ê·  ì†ì‹¤ ë°˜í™˜

# í•˜ë‹¨ crop í•¨ìˆ˜ ì •ì˜
def crop_bottom(img):
    img = img.resize((RESIZE_WIDTH, RESIZE_HEIGHT))  # ë¨¼ì € í¬ê¸° ì¡°ì •
    return img.crop((0, 120, 320, 180))  # í•˜ë‹¨ 60í”½ì…€ ì˜ì—­ crop

# í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate(model, loader, criterion, device):
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    total_loss = 0.0  # ì†ì‹¤ ëˆ„ì ê°’
    preds, labels = [], []  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    with torch.no_grad():  # í‰ê°€ ì‹œ ê·¸ë˜í”„ ìƒì„± ë¹„í™œì„±í™”
        for imgs, angles in tqdm(loader, desc="Evaluating", leave=False):  # ë°°ì¹˜ë³„ í‰ê°€ ë£¨í”„
            imgs, angles = imgs.to(device), angles.to(device)  # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            output = model(imgs).squeeze()  # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
            loss = criterion(output, angles)  # ì†ì‹¤ ê³„ì‚°
            total_loss += loss.item() * imgs.size(0)  # ì†ì‹¤ ëˆ„ì 
            preds.extend(output.cpu().numpy())  # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            labels.extend(angles.cpu().numpy())  # ì‹¤ì œ ê°ë„ ì €ì¥
    return total_loss / len(loader.dataset), preds, labels  # í‰ê·  ì†ì‹¤, ì˜ˆì¸¡, ì‹¤ì œ ë°˜í™˜


if __name__ == '__main__':
    train_transform = transforms.Compose([
        transforms.Lambda(crop_bottom),
        transforms.Resize((60, 320)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])

    valid_transform = transforms.Compose([
        transforms.Lambda(crop_bottom),
        transforms.Resize((60, 320)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])

    df = pd.read_csv(LABELS_CSV)
    n_samples = len(df)

    indices = torch.randperm(n_samples).tolist()
    train_size = int(0.8 * n_samples)
    train_indices = indices[:train_size]
    valid_indices = indices[train_size:]

    train_set = SteeringDataset(
        LABELS_CSV,
        DATASET_DIR,
        transform=train_transform,
        use_random_flip=True,
        indices=train_indices
    )

    valid_set = SteeringDataset(
        LABELS_CSV,
        DATASET_DIR,
        transform=valid_transform,
        use_random_flip=False,
        indices=valid_indices
    )

    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, pin_memory=True)
    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False,
                              num_workers=NUM_WORKERS, pin_memory=True)
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # í•™ìŠµ ë””ë°”ì´ìŠ¤ ì„¤ì •
    model = SteeringModel().to(device)  # ëª¨ë¸ ì´ˆê¸°í™” ë° ë””ë°”ì´ìŠ¤ í• ë‹¹
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)  # Adam ì˜µí‹°ë§ˆì´ì € (weight decay ì¶”ê°€)
    criterion = nn.MSELoss()  # ì†ì‹¤ í•¨ìˆ˜: í‰ê· ì œê³±ì˜¤ì°¨

    # LR scheduler ì„¤ì • (validation loss ê¸°ì¤€ ReduceLROnPlateau)
    scheduler = None
    if REDUCE_LR_ON_PLATEAU:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=REDUCE_LR_FACTOR, patience=REDUCE_LR_PATIENCE, min_lr=MIN_LR, verbose=True)

    save_dir = get_unique_train_folder()  # ê³ ìœ  í•™ìŠµ ê²°ê³¼ í´ë” ìƒì„±
    log_path = os.path.join(save_dir, 'log.csv')  # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    with open(log_path, 'w') as f:  # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
        f.write('epoch,train_loss,valid_loss\n')  # í—¤ë” ì‘ì„±
    best_loss = float('inf')  # ì´ˆê¸° ìµœì  ì†ì‹¤ê°’ ë¬´í•œëŒ€
    patience = EARLY_STOPPING_PATIENCE     
    no_improve_count = 0      

    # í•™ìŠµ ë£¨í”„ ì‹œì‘
    for epoch in range(1, EPOCHS + 1):
        print(f"Epoch {epoch}/{EPOCHS}")  # í˜„ì¬ ì—í­ ì¶œë ¥
        # train loop (clip gradì„ ì „ë‹¬í•˜ì—¬ í­ì£¼ ì°¨ë‹¨)
        train_loss = train(model, train_loader, optimizer, criterion, device, clip_grad=CLIP_GRAD_NORM)
        valid_loss, preds, labels = evaluate(model, valid_loader, criterion, device)  # ê²€ì¦ ì†ì‹¤ ê³„ì‚°
        # scheduler: ReduceLROnPlateau ì‚¬ìš© ì‹œ validation lossë¡œ ìŠ¤í…
        if scheduler is not None:
            scheduler.step(valid_loss)
        with open(log_path, 'a') as f:  # ë¡œê·¸ ê¸°ë¡ ì¶”ê°€
            f.write(f"{epoch},{train_loss:.6f},{valid_loss:.6f}\n")
        # í˜„ì¬ learning rate ì¶œë ¥
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Train Loss: {train_loss:.4f} | valid Loss: {valid_loss:.4f} | lr: {current_lr:.2e}")

        # í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ ì´ì „ ì²´í¬í¬ì¸íŠ¸ ì œê±°
        ckpt_path = os.path.join(save_dir, f"checkpoint_epoch{epoch}.pth")
        torch.save(model.state_dict(), ckpt_path)  # í˜„ì¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
        if epoch > 1:  # ì´ì „ ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ ì‹œ
            prev_ckpt = os.path.join(save_dir, f"checkpoint_epoch{epoch - 1}.pth")
            if os.path.exists(prev_ckpt):
                os.remove(prev_ckpt)  # ì´ì „ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ

        if valid_loss < best_loss:  # ìƒˆë¡œìš´ ìµœì  ëª¨ë¸ì¸ ê²½ìš°
            best_loss = valid_loss              # ìµœì  ì†ì‹¤ ê°±ì‹ 
            no_improve_count = 0               # ê°œì„ ëìœ¼ë‹ˆ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pth"))  # ìµœì  ëª¨ë¸ ì €ì¥
        else:
            no_improve_count += 1              # ê°œì„  ì•ˆ ë¨ â†’ ì¹´ìš´íŠ¸ ì¦ê°€
            print(f"no improvement count: {no_improve_count}/{patience}")

        torch.save(model.state_dict(), os.path.join(save_dir, "last_model.pth"))  # ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥

        # ğŸ”¥ Early Stopping ì²´í¬
        if no_improve_count >= patience:
            print(f"Early stopping triggered at epoch {epoch}")
            break

    # ì†ì‹¤ ê³¡ì„  í”Œë¡œíŒ…
    log_df = pd.read_csv(log_path, index_col=False)   # CSV ì½ê¸° (epochì´ ì¸ë±ìŠ¤ë¡œ ì¡íˆì§€ ì•Šë„ë¡ ì„¤ì •)
    log_df.columns = log_df.columns.str.strip()      # ì—´ ì´ë¦„ì˜ ì•ë’¤ ê³µë°± ì œê±°
    log_df = log_df.astype(float)                    # ëª¨ë“  ì—´ì„ ì‹¤ìˆ˜í˜•(float)ìœ¼ë¡œ ë³€í™˜
    
    plt.figure(figsize=(10,5))                       # í”Œë¡¯ í¬ê¸° ì„¤ì • (10x5 ì¸ì¹˜)
    
    plt.plot(log_df['epoch'].to_numpy(),             # xì¶•: epoch
             log_df['train_loss'].to_numpy(),        # yì¶•: í•™ìŠµ ì†ì‹¤
             label='Training Loss')                  # ë¼ë²¨: Training Loss
    
    plt.plot(log_df['epoch'].to_numpy(),             # xì¶•: epoch
             log_df['valid_loss'].to_numpy(),         # yì¶•: ê²€ì¦ ì†ì‹¤
             label='Validation Loss', linestyle='--')# ë¼ë²¨: Validation Loss (ì ì„ )
    
    plt.xlabel('Epoch')                              # xì¶• ë ˆì´ë¸”
    plt.ylabel('Loss')                               # yì¶• ë ˆì´ë¸”
    plt.title('Training Loss vs Validation Loss')    # ê·¸ë˜í”„ ì œëª©
    plt.legend()                                     # ë²”ë¡€ í‘œì‹œ
    plt.grid()                                       # ê·¸ë¦¬ë“œ í‘œì‹œ
    plt.savefig(os.path.join(save_dir, 'loss_curve.png'))  # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    plt.close()                                      # í”Œë¡¯ ë‹«ê¸°
